# Football Manager Analytics Project
Nordeus Data Engineering Challenge

## Overview

This project explores how gamers react football manager game released by the 'Nordeus' company. The dataset includes real-time events generated by gamers, providing insights at the country, date, and individual levels.

### Data Cleaning

The initial phase involves cleaning incorrect or invalid events using the `cleaning_data.py` file. The data cleaning process addresses the following aspects:

1. Check for duplicate events (events occurring at the same time by the same user).
2. Discard events of a user that happened before the registration event of the same user.
3. Discard transactions not included in a gamer session.
4. Discard consecutive events of the same type generated by the same user.
5. Remove events with invalid properties (e.g., 'device_os' not iOS, Android, or Web).

### Data Transformation

The data is transformed into a desired and reliable format using `user_level_stat.py` and `game_level_stat.py` files. Two data frames are created to provide the necessary information for insertion into the database.

### Database Structure

The data is prepared for insertion into a PostgreSQL database representing a data warehouse (`processed_data.py`). Three dimensional tables are anticipated, with tables for 'gamer', 'time', and 'country'. Additionally, two data frames serve as fact tables for two data marts - 'user_level_stat' (dimensions: consumer and time) and 'game_level_stat' (dimensions: country and time).

### SQL Queries

All necessary SQL queries are located in the `sql_queries.py` file.

### Prerequisites

 - Python
 - PostgreSQL
 - Swagger
 
 In addition, all libraries used within the project are written in the `requirements.txt` file. It is worth noting that the REST API is well documented, using the Swagger tool.

### How to use app?

After installing all the necessary technologies and program libraries, the following steps describe how the application should be run and used.
 1. Start the Flask server with `python .\api.py` command (by default the server starts on port 105)
 2. Execute `processed_data.py` to process input file and store target data model into database
 3. Open a web browser on port 105, where a documented application API will appear, available for querying data

## Conclusion

In summary, the project itself can be done in a handful of ways, in different technologies, but what is much more important, is the way in which solving a challenge is approached. While working on the project, I noticed several possible approaches.

So, the first one is the one I chose, among other things, because the dataset is not that big. In essence, it comes down to passing the dataset through several stages, process called ECTL (Extract Clean Transform and Load). A very important thing in this approach is the emphasis on data aggregation, because data is aggregated at a low level for the purpose of analysis. In the final phase of this approach, the data is loaded into some kind of the data warehouse. The main disadvantage of this approach is that the complete process cannot be parallelized in this way, precisely because at some point the data is aggregated.

Second approach would be not to aggregate the data at all. The emphasis would be on the parallelization of the process of data cleaning and loading in a relational database. In this way, the process of data cleaning would be much more accelerated, but it would be significantly more difficult to get some information in the case of data analysis.

Finally, the last approach that should be mentioned is the case if the dataset is a quite large to process, using some kind of a tool (e.g. Apache Spark). This is a case that would be practical in much larger projects than this, primarily in terms of the amount of data that would be processed in a distributed manner.










